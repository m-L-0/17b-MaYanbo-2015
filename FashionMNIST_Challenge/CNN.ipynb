{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用CNN算法对图片进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下载并加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"data/fashion\", one_hot=True)     #下载并加载mnist数据\n",
    "x = tf.placeholder(tf.float32, [None, 784])                        #输入的数据占位符\n",
    "y_actual = tf.placeholder(tf.float32, shape=[None, 10])            #输入的标签占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, dtype='uint8', is_row_iamge=False):\n",
    "        '''数据集\n",
    "        \n",
    "        Args:\n",
    "            dtype: uint8 或 float32，uint8：每个像素值的范围是[0, 255];float32像素值范围是[0., 1.]\n",
    "            is_row_image: 是否将3维图片展开成1维\n",
    "        '''\n",
    "        images = np.fromfile('./images/test_image.bin', dtype=np.uint8).reshape(-1, 28, 28, 1)\n",
    "        print(images.shape)\n",
    "        if dtype == 'uint8':\n",
    "            self.images = images\n",
    "        else:\n",
    "            images = images.astype(np.float32) / 255.\n",
    "            self.images = images\n",
    "        if is_row_iamge:\n",
    "            self.images = images.reshape([-1, 784])\n",
    "        self.num_of_images = 6500\n",
    "        self.offset = 0\n",
    "        print('共6500张图片')\n",
    "\n",
    "    def next_batch(self, batch_size=50):\n",
    "        # 返回False表示以及没有样本\n",
    "        # 注意：最后一个批次可能不足batch_size 所以推荐选择6500可以整除的batch_size\n",
    "        if (self.offset + batch_size) <= self.num_of_images:\n",
    "            self.offset += batch_size\n",
    "            return self.images[self.offset-batch_size : self.offset]\n",
    "        elif self.offset < self.num_of_images:\n",
    "            return self.images[self.offset : ]\n",
    "        else:\n",
    "            False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 参数保存目录\\nFLAGS = tf.app.flags.FLAGS\\n# Basic model parameters.\\ntf.app.flags.DEFINE_string(\\'cnn_path\\', \\'/home/bigwork/\\', \"\"\"存放模型的目录\"\"\")\\n\\ntf.app.flags.DEFINE_string(\\'cnn_parameters\\', \\'mnist\\',\"\"\"模型的名称\"\"\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 参数保存目录\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "# Basic model parameters.\n",
    "tf.app.flags.DEFINE_string('cnn_path', '/home/bigwork/', \"\"\"存放模型的目录\"\"\")\n",
    "\n",
    "tf.app.flags.DEFINE_string('cnn_parameters', 'mnist',\"\"\"模型的名称\"\"\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义四个函数，分别用于初始化权值W，初始化偏置项b, 构建卷积层和构建池化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个函数，用于初始化所有的权值 W\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "#定义一个函数，用于初始化所有的偏置项 b\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "  \n",
    "#定义一个函数，用于构建卷积层\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "#定义一个函数，用于构建池化层\n",
    "def max_pool(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#模型文件所在的文件夹，是否存在，如果不存在，则创建文件夹\\nckpt = tf.train.latest_checkpoint(FLAGS.my_list)\\nif not ckpt:\\n    if not os.path.exists(FLAGS.my_list):\\n        os.mkdir(FLAGS.my_list)\\nX_ = tf.placeholder(tf.float32, [None, 784])\\ny_ = tf.placeholder(tf.float32, [None, 10])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#模型文件所在的文件夹，是否存在，如果不存在，则创建文件夹\n",
    "ckpt = tf.train.latest_checkpoint(FLAGS.my_list)\n",
    "if not ckpt:\n",
    "    if not os.path.exists(FLAGS.my_list):\n",
    "        os.mkdir(FLAGS.my_list)\n",
    "X_ = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建网络。整个网络由两个卷积层（包含激活层和池化层），一个全连接层，一个dropout层和一个softmax层组成。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建网络\n",
    "x_image = tf.reshape(x, [-1,28,28,1])         #转换输入数据shape,以便于用于网络中\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])      \n",
    "b_conv1 = bias_variable([32])       \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)     #第一个卷积层\n",
    "h_pool1 = max_pool(h_conv1)                                  #第一个池化层\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)      #第二个卷积层\n",
    "h_pool2 = max_pool(h_conv2)                                   #第二个池化层\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])              #reshape成向量\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)    #第一个全连接层\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\") \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)                  #dropout层\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_predict=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)   #softmax层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mmm/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step  0 training accuracy  0.3\n",
      "step  100 training accuracy  0.12\n",
      "step  200 training accuracy  0.18\n",
      "step  300 training accuracy  0.06\n",
      "step  400 training accuracy  0.02\n",
      "step  500 training accuracy  0.14\n",
      "step  600 training accuracy  0.24\n",
      "step  700 training accuracy  0.24\n",
      "step  800 training accuracy  0.24\n",
      "step  900 training accuracy  0.28\n",
      "step  1000 training accuracy  0.24\n",
      "step  1100 training accuracy  0.18\n",
      "step  1200 training accuracy  0.46\n",
      "step  1300 training accuracy  0.46\n",
      "step  1400 training accuracy  0.38\n",
      "step  1500 training accuracy  0.5\n",
      "step  1600 training accuracy  0.52\n",
      "step  1700 training accuracy  0.6\n",
      "step  1800 training accuracy  0.6\n",
      "step  1900 training accuracy  0.52\n",
      "step  2000 training accuracy  0.48\n",
      "step  2100 training accuracy  0.48\n",
      "step  2200 training accuracy  0.54\n",
      "step  2300 training accuracy  0.78\n",
      "step  2400 training accuracy  0.48\n",
      "step  2500 training accuracy  0.66\n",
      "step  2600 training accuracy  0.7\n",
      "step  2700 training accuracy  0.56\n",
      "step  2800 training accuracy  0.5\n",
      "step  2900 training accuracy  0.52\n",
      "step  3000 training accuracy  0.68\n",
      "step  3100 training accuracy  0.64\n",
      "step  3200 training accuracy  0.64\n",
      "step  3300 training accuracy  0.5\n",
      "step  3400 training accuracy  0.48\n",
      "step  3500 training accuracy  0.64\n",
      "step  3600 training accuracy  0.66\n",
      "step  3700 training accuracy  0.62\n",
      "step  3800 training accuracy  0.56\n",
      "step  3900 training accuracy  0.66\n",
      "step  4000 training accuracy  0.64\n",
      "step  4100 training accuracy  0.5\n",
      "step  4200 training accuracy  0.78\n",
      "step  4300 training accuracy  0.64\n",
      "step  4400 training accuracy  0.68\n",
      "step  4500 training accuracy  0.7\n",
      "step  4600 training accuracy  0.78\n",
      "step  4700 training accuracy  0.66\n",
      "step  4800 training accuracy  0.62\n",
      "step  4900 training accuracy  0.66\n",
      "step  5000 training accuracy  0.82\n",
      "step  5100 training accuracy  0.74\n",
      "step  5200 training accuracy  0.86\n",
      "step  5300 training accuracy  0.8\n",
      "step  5400 training accuracy  0.7\n",
      "step  5500 training accuracy  0.72\n",
      "step  5600 training accuracy  0.8\n",
      "step  5700 training accuracy  0.68\n",
      "step  5800 training accuracy  0.84\n",
      "step  5900 training accuracy  0.7\n",
      "step  6000 training accuracy  0.7\n",
      "step  6100 training accuracy  0.66\n",
      "step  6200 training accuracy  0.68\n",
      "step  6300 training accuracy  0.64\n",
      "step  6400 training accuracy  0.62\n",
      "step  6500 training accuracy  0.72\n",
      "step  6600 training accuracy  0.64\n",
      "step  6700 training accuracy  0.66\n",
      "step  6800 training accuracy  0.78\n",
      "step  6900 training accuracy  0.66\n",
      "step  7000 training accuracy  0.78\n",
      "step  7100 training accuracy  0.64\n",
      "step  7200 training accuracy  0.7\n",
      "step  7300 training accuracy  0.68\n",
      "step  7400 training accuracy  0.76\n",
      "step  7500 training accuracy  0.7\n",
      "step  7600 training accuracy  0.72\n",
      "step  7700 training accuracy  0.8\n",
      "step  7800 training accuracy  0.74\n",
      "step  7900 training accuracy  0.66\n",
      "step  8000 training accuracy  0.68\n",
      "step  8100 training accuracy  0.7\n",
      "step  8200 training accuracy  0.72\n",
      "step  8300 training accuracy  0.64\n",
      "step  8400 training accuracy  0.8\n",
      "step  8500 training accuracy  0.76\n",
      "step  8600 training accuracy  0.74\n",
      "step  8700 training accuracy  0.88\n",
      "step  8800 training accuracy  0.76\n",
      "step  8900 training accuracy  0.8\n",
      "step  9000 training accuracy  0.8\n",
      "step  9100 training accuracy  0.68\n",
      "step  9200 training accuracy  0.76\n",
      "step  9300 training accuracy  0.82\n",
      "step  9400 training accuracy  0.82\n",
      "step  9500 training accuracy  0.72\n",
      "step  9600 training accuracy  0.64\n",
      "step  9700 training accuracy  0.7\n",
      "step  9800 training accuracy  0.72\n",
      "step  9900 training accuracy  0.68\n",
      "step  10000 training accuracy  0.8\n",
      "step  10100 training accuracy  0.82\n",
      "step  10200 training accuracy  0.76\n",
      "step  10300 training accuracy  0.78\n",
      "step  10400 training accuracy  0.76\n",
      "step  10500 training accuracy  0.76\n",
      "step  10600 training accuracy  0.86\n",
      "step  10700 training accuracy  0.84\n",
      "step  10800 training accuracy  0.88\n",
      "step  10900 training accuracy  0.82\n",
      "step  11000 training accuracy  0.82\n",
      "step  11100 training accuracy  0.66\n",
      "step  11200 training accuracy  0.68\n",
      "step  11300 training accuracy  0.82\n",
      "step  11400 training accuracy  0.86\n",
      "step  11500 training accuracy  0.76\n",
      "step  11600 training accuracy  0.82\n",
      "step  11700 training accuracy  0.7\n",
      "step  11800 training accuracy  0.84\n",
      "step  11900 training accuracy  0.86\n",
      "step  12000 training accuracy  0.82\n",
      "step  12100 training accuracy  0.68\n",
      "step  12200 training accuracy  0.68\n",
      "step  12300 training accuracy  0.8\n",
      "step  12400 training accuracy  0.72\n",
      "step  12500 training accuracy  0.74\n",
      "step  12600 training accuracy  0.78\n",
      "step  12700 training accuracy  0.9\n",
      "step  12800 training accuracy  0.82\n",
      "step  12900 training accuracy  0.86\n",
      "step  13000 training accuracy  0.76\n",
      "step  13100 training accuracy  0.82\n",
      "step  13200 training accuracy  0.72\n",
      "step  13300 training accuracy  0.7\n",
      "step  13400 training accuracy  0.84\n",
      "step  13500 training accuracy  0.86\n",
      "step  13600 training accuracy  0.66\n",
      "step  13700 training accuracy  0.82\n",
      "step  13800 training accuracy  0.64\n",
      "step  13900 training accuracy  0.84\n",
      "step  14000 training accuracy  0.72\n",
      "step  14100 training accuracy  0.72\n",
      "step  14200 training accuracy  0.8\n",
      "step  14300 training accuracy  0.82\n",
      "step  14400 training accuracy  0.74\n",
      "step  14500 training accuracy  0.76\n",
      "step  14600 training accuracy  0.84\n",
      "step  14700 training accuracy  0.8\n",
      "step  14800 training accuracy  0.66\n",
      "step  14900 training accuracy  0.78\n",
      "step  15000 training accuracy  0.82\n",
      "step  15100 training accuracy  0.78\n",
      "step  15200 training accuracy  0.76\n",
      "step  15300 training accuracy  0.78\n",
      "step  15400 training accuracy  0.78\n",
      "step  15500 training accuracy  0.92\n",
      "step  15600 training accuracy  0.8\n",
      "step  15700 training accuracy  0.7\n",
      "step  15800 training accuracy  0.78\n",
      "step  15900 training accuracy  0.74\n",
      "step  16000 training accuracy  0.84\n",
      "step  16100 training accuracy  0.82\n",
      "step  16200 training accuracy  0.8\n",
      "step  16300 training accuracy  0.74\n",
      "step  16400 training accuracy  0.8\n",
      "step  16500 training accuracy  0.8\n",
      "step  16600 training accuracy  0.82\n",
      "step  16700 training accuracy  0.8\n",
      "step  16800 training accuracy  0.8\n",
      "step  16900 training accuracy  0.78\n",
      "step  17000 training accuracy  0.62\n",
      "step  17100 training accuracy  0.8\n",
      "step  17200 training accuracy  0.78\n",
      "step  17300 training accuracy  0.76\n",
      "step  17400 training accuracy  0.8\n",
      "step  17500 training accuracy  0.82\n",
      "step  17600 training accuracy  0.74\n",
      "step  17700 training accuracy  0.8\n",
      "step  17800 training accuracy  0.74\n",
      "step  17900 training accuracy  0.9\n",
      "step  18000 training accuracy  0.84\n",
      "step  18100 training accuracy  0.78\n",
      "step  18200 training accuracy  0.86\n",
      "step  18300 training accuracy  0.76\n",
      "step  18400 training accuracy  0.78\n",
      "step  18500 training accuracy  0.88\n",
      "step  18600 training accuracy  0.76\n",
      "step  18700 training accuracy  0.84\n",
      "step  18800 training accuracy  0.64\n",
      "step  18900 training accuracy  0.72\n",
      "step  19000 training accuracy  0.8\n",
      "step  19100 training accuracy  0.82\n",
      "step  19200 training accuracy  0.72\n",
      "step  19300 training accuracy  0.76\n",
      "step  19400 training accuracy  0.72\n",
      "step  19500 training accuracy  0.86\n",
      "step  19600 training accuracy  0.74\n",
      "step  19700 training accuracy  0.88\n",
      "step  19800 training accuracy  0.8\n",
      "step  19900 training accuracy  0.8\n",
      "step  20000 training accuracy  0.72\n",
      "step  20100 training accuracy  0.76\n",
      "step  20200 training accuracy  0.84\n",
      "step  20300 training accuracy  0.78\n",
      "step  20400 training accuracy  0.84\n",
      "step  20500 training accuracy  0.82\n",
      "step  20600 training accuracy  0.8\n",
      "step  20700 training accuracy  0.78\n",
      "step  20800 training accuracy  0.88\n",
      "step  20900 training accuracy  0.8\n",
      "step  21000 training accuracy  0.88\n",
      "step  21100 training accuracy  0.84\n",
      "step  21200 training accuracy  0.8\n",
      "step  21300 training accuracy  0.84\n",
      "step  21400 training accuracy  0.84\n",
      "step  21500 training accuracy  0.8\n",
      "step  21600 training accuracy  0.86\n",
      "step  21700 training accuracy  0.8\n",
      "step  21800 training accuracy  0.86\n",
      "step  21900 training accuracy  0.72\n",
      "step  22000 training accuracy  0.76\n",
      "step  22100 training accuracy  0.84\n",
      "step  22200 training accuracy  0.8\n",
      "step  22300 training accuracy  0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  22400 training accuracy  0.86\n",
      "step  22500 training accuracy  0.82\n",
      "step  22600 training accuracy  0.82\n",
      "step  22700 training accuracy  0.86\n",
      "step  22800 training accuracy  0.8\n",
      "step  22900 training accuracy  0.76\n",
      "step  23000 training accuracy  0.76\n",
      "step  23100 training accuracy  0.84\n",
      "step  23200 training accuracy  0.7\n",
      "step  23300 training accuracy  0.84\n",
      "step  23400 training accuracy  0.76\n",
      "step  23500 training accuracy  0.8\n",
      "step  23600 training accuracy  0.9\n",
      "step  23700 training accuracy  0.84\n",
      "step  23800 training accuracy  0.82\n",
      "step  23900 training accuracy  0.9\n",
      "step  24000 training accuracy  0.76\n",
      "step  24100 training accuracy  0.84\n",
      "step  24200 training accuracy  0.82\n",
      "step  24300 training accuracy  0.92\n",
      "step  24400 training accuracy  0.72\n",
      "step  24500 training accuracy  0.76\n",
      "step  24600 training accuracy  0.8\n",
      "step  24700 training accuracy  0.84\n",
      "step  24800 training accuracy  0.84\n",
      "step  24900 training accuracy  0.84\n",
      "(6500, 28, 28, 1)\n",
      "共6500张图片\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#\\u3000预测值中最大值（１）即分类结果，是否等于原始标签中的（１）的位置。argmax()取最大值所在的下标\\nz = tf.argmax(y_predict, 1)\\ntest_acc_sum = tf.Variable(0.0)\\nbatch_acc = tf.placeholder(tf.float32)\\nnew_test_acc_sum = tf.add(test_acc_sum, batch_acc)\\nupdate = tf.assign(test_acc_sum, new_test_acc_sum)\\nsaver=tf.train.Saver(max_to_keep=2)\\n# 定义了变量必须要初始化，或者下面形式\\nwith tf.Session() as sess:\\n    sess.run(tf.global_variables_initializer())\\n# 或者某个变量单独初始化 如：\\n    check_point_path = \\'/home/bigwork/\\' # 保存好模型的文件路径\\n    ckpt = tf.train.get_checkpoint_state(checkpoint_dir=check_point_path)\\n    #saver.restore(sess,ckpt.model_checkpoint_path)\\n\\n    Y = np.zeros(10000)\\n    X_batch, y_batch = mnist.test.next_batch(batch_size=10000)\\n    Ytemp = y_predict.eval(feed_dict={x : X_batch[:1000], keep_prob: 1.0})\\n    for i in range(1000):\\n            #生成0-9标签\\n        Y[i] = np.argmax(Ytemp[i])\\n        # print(Y[i])\\n    # print(\"test accuracy %g\" % accuracy.eval(feed_dict={X_: X_batch, y_: y_batch, keep_prob: 1.0}))\\nfp = open(\"test.txt\", \"w+\")\\nfor i in range(10000):\\n    fp.write(str(int(Y[i]))+\"\\n\")\\nfp.close()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_actual*tf.log(y_predict))     #交叉熵\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy)    #梯度下降法\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict,1), tf.argmax(y_actual,1))    \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))                 #精确度计算\n",
    "sess=tf.InteractiveSession()                          \n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(25000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:                  #训练100次，验证一次\n",
    "    train_acc = accuracy.eval(feed_dict={x:batch[0], y_actual: batch[1], keep_prob: 1.0})\n",
    "    print ('step ',i, 'training accuracy ',train_acc)\n",
    "    train_step.run(feed_dict={x: batch[0], y_actual: batch[1], keep_prob: 0.5})\n",
    "\n",
    "\n",
    "check_point_path = '/home/bigwork/' # 保存好模型的文件路径\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_dir=check_point_path)\n",
    "    #saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "Y = np.zeros(6500)\n",
    "#X_batch, y_batch = mnist.test.next_batch(batch_size=10000)\n",
    "images = Dataset(dtype='float32',is_row_iamge=True)\n",
    "for j in range(130):\n",
    "    X_batch = images.next_batch(batch_size=50)\n",
    "    X_batch = X_batch.reshape((50, 784))\n",
    "    Ytemp = y_predict.eval(feed_dict={x : X_batch[:], keep_prob: 1.0})\n",
    "    for i in range(50):\n",
    "        #生成0-9标签\n",
    "        Y[j*50+i] = np.argmax(Ytemp[i])\n",
    "    # print(Y[i])\n",
    "# print(\"test accuracy %g\" % accuracy.eval(feed_dict={X_: X_batch, y_: y_batch, keep_prob: 1.0}))\n",
    "fp = open(\"test0.txt\", \"w+\")\n",
    "for i in range(6500):\n",
    "    fp.write(str(int(Y[i]))+\"\\n\")\n",
    "fp.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
